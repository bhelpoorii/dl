import pandas as pd

# Read the CSV file
df = pd.read_csv("your_file.csv")

# Print the entire content
print(df)

# Print first 5 rows (optional)
print(df.head())
## for numerical mapping 
#df["target"] = df["target"].map({"No": 0, "Yes": 1})


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.datasets import load_breast_cancer
import warnings
warnings.filterwarnings('ignore')

# =====================================================
# Neural Network Class
# =====================================================
class NeuralNetwork:
    def __init__(self, input_size, hidden_size, output_size, dropout_rate=0.1):
        self.W1 = np.random.randn(input_size, hidden_size) * 0.01
        self.b1 = np.zeros((1, hidden_size))
        self.W2 = np.random.randn(hidden_size, output_size) * 0.01
        self.b2 = np.zeros((1, output_size))

        self.dropout_rate = dropout_rate
        self.training = True

    def sigmoid(self, z):
        z = np.clip(z, -500, 500)
        return 1 / (1 + np.exp(-z))

    def sigmoid_derivative(self, z):
        return z * (1 - z)

    def dropout(self, X):
        if self.training and self.dropout_rate > 0:
            mask = np.random.binomial(1, 1 - self.dropout_rate, X.shape) / (1 - self.dropout_rate)
            return X * mask, mask
        return X, np.ones_like(X)

    def forward(self, X):
        self.z1 = np.dot(X, self.W1) + self.b1
        self.a1 = self.sigmoid(self.z1)

        self.a1_dropout, self.dropout_mask = self.dropout(self.a1)

        self.z2 = np.dot(self.a1_dropout, self.W2) + self.b2
        self.a2 = self.sigmoid(self.z2)
        return self.a2

    def compute_loss(self, y_true, y_pred):
        epsilon = 1e-15
        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)
        return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))

    def backward(self, X, y, y_pred):
        m = X.shape[0]

        dz2 = y_pred - y
        dW2 = (1/m) * np.dot(self.a1_dropout.T, dz2)
        db2 = (1/m) * np.sum(dz2, axis=0, keepdims=True)

        da1 = np.dot(dz2, self.W2.T)
        if self.training and self.dropout_rate > 0:
            da1 = da1 * self.dropout_mask

        dz1 = da1 * self.sigmoid_derivative(self.a1)
        dW1 = (1/m) * np.dot(X.T, dz1)
        db1 = (1/m) * np.sum(dz1, axis=0, keepdims=True)

        return dW1, db1, dW2, db2

    def set_training(self, training):
        self.training = training


# =====================================================
# Optimizers
# =====================================================
class SGDOptimizer:
    def __init__(self, learning_rate=0.01):
        self.learning_rate = learning_rate
        self.name = "SGD"

    def update(self, model, dW1, db1, dW2, db2):
        model.W1 -= self.learning_rate * dW1
        model.b1 -= self.learning_rate * db1
        model.W2 -= self.learning_rate * dW2
        model.b2 -= self.learning_rate * db2


class MomentumOptimizer:
    def __init__(self, learning_rate=0.01, momentum=0.9):
        self.learning_rate = learning_rate
        self.momentum = momentum
        self.name = "Momentum"

        self.vW1 = 0; self.vb1 = 0
        self.vW2 = 0; self.vb2 = 0

    def update(self, model, dW1, db1, dW2, db2):
        self.vW1 = self.momentum * self.vW1 + self.learning_rate * dW1
        self.vb1 = self.momentum * self.vb1 + self.learning_rate * db1
        self.vW2 = self.momentum * self.vW2 + self.learning_rate * dW2
        self.vb2 = self.momentum * self.vb2 + self.learning_rate * db2

        model.W1 -= self.vW1
        model.b1 -= self.vb1
        model.W2 -= self.vW2
        model.b2 -= self.vb2


class AdaGradOptimizer:
    def __init__(self, learning_rate=0.01, epsilon=1e-8):
        self.learning_rate = learning_rate
        self.epsilon = epsilon
        self.name = "AdaGrad"

        self.GW1 = 0; self.Gb1 = 0
        self.GW2 = 0; self.Gb2 = 0

    def update(self, model, dW1, db1, dW2, db2):
        self.GW1 += dW1 ** 2
        self.Gb1 += db1 ** 2
        self.GW2 += dW2 ** 2
        self.Gb2 += db2 ** 2

        model.W1 -= (self.learning_rate / (np.sqrt(self.GW1) + self.epsilon)) * dW1
        model.b1 -= (self.learning_rate / (np.sqrt(self.Gb1) + self.epsilon)) * db1
        model.W2 -= (self.learning_rate / (np.sqrt(self.GW2) + self.epsilon)) * dW2
        model.b2 -= (self.learning_rate / (np.sqrt(self.Gb2) + self.epsilon)) * db2


class AdamOptimizer:
    def __init__(self, learning_rate=0.01, beta1=0.9, beta2=0.999, epsilon=1e-8):
        self.learning_rate = learning_rate
        self.beta1 = beta1
        self.beta2 = beta2
        self.epsilon = epsilon
        self.t = 0
        self.name = "Adam"

        self.mW1 = 0; self.vW1 = 0
        self.mb1 = 0; self.vb1 = 0
        self.mW2 = 0; self.vW2 = 0
        self.mb2 = 0; self.vb2 = 0

    def update(self, model, dW1, db1, dW2, db2):
        self.t += 1

        self.mW1 = self.beta1 * self.mW1 + (1 - self.beta1) * dW1
        self.mb1 = self.beta1 * self.mb1 + (1 - self.beta1) * db1
        self.mW2 = self.beta1 * self.mW2 + (1 - self.beta1) * dW2
        self.mb2 = self.beta1 * self.mb2 + (1 - self.beta1) * db2

        self.vW1 = self.beta2 * self.vW1 + (1 - self.beta2) * (dW1 ** 2)
        self.vb1 = self.beta2 * self.vb1 + (1 - self.beta2) * (db1 ** 2)
        self.vW2 = self.beta2 * self.vW2 + (1 - self.beta2) * (dW2 ** 2)
        self.vb2 = self.beta2 * self.vb2 + (1 - self.beta2) * (db2 ** 2)

        mW1_corr = self.mW1 / (1 - self.beta1 ** self.t)
        mb1_corr = self.mb1 / (1 - self.beta1 ** self.t)
        mW2_corr = self.mW2 / (1 - self.beta1 ** self.t)
        mb2_corr = self.mb2 / (1 - self.beta1 ** self.t)

        vW1_corr = self.vW1 / (1 - self.beta2 ** self.t)
        vb1_corr = self.vb1 / (1 - self.beta2 ** self.t)
        vW2_corr = self.vW2 / (1 - self.beta2 ** self.t)
        vb2_corr = self.vb2 / (1 - self.beta2 ** self.t)

        model.W1 -= self.learning_rate * mW1_corr / (np.sqrt(vW1_corr) + self.epsilon)
        model.b1 -= self.learning_rate * mb1_corr / (np.sqrt(vb1_corr) + self.epsilon)
        model.W2 -= self.learning_rate * mW2_corr / (np.sqrt(vW2_corr) + self.epsilon)
        model.b2 -= self.learning_rate * mb2_corr / (np.sqrt(vb2_corr) + self.epsilon)


# =====================================================
# Early Stopping + Fit Detection
# =====================================================
class EarlyStopping:
    def __init__(self, patience=10):
        self.patience = patience
        self.best_loss = np.inf
        self.counter = 0

    def __call__(self, val_loss):
        if val_loss < self.best_loss:
            self.best_loss = val_loss
            self.counter = 0
        else:
            self.counter += 1
        return self.counter >= self.patience


def detect_overfitting(train_losses, val_losses):
    if len(train_losses) < 10:
        return "Unknown"
    final_gap = val_losses[-1] - train_losses[-1]
    if final_gap > 0.1:
        return "Overfitting"
    elif train_losses[-1] > 0.6:
        return "Underfitting"
    else:
        return "Good Fit"


# =====================================================
# Training
# =====================================================
def train_model(X_train, y_train, X_val, y_val, dropout_rate=0.0,
                use_early_stopping=False, optimizer_type="AdaGrad"):

    model = NeuralNetwork(X_train.shape[1], 10, 1, dropout_rate)

    # Pick optimizer
    if optimizer_type == "SGD":
        optimizer = SGDOptimizer(learning_rate=0.1)
    elif optimizer_type == "Momentum":
        optimizer = MomentumOptimizer(learning_rate=0.1)
    elif optimizer_type == "AdaGrad":
        optimizer = AdaGradOptimizer(learning_rate=0.1)
    elif optimizer_type == "Adam":
        optimizer = AdamOptimizer(learning_rate=0.01)
    else:
        raise ValueError("Unknown optimizer")

    early_stopping = EarlyStopping(patience=20) if use_early_stopping else None

    train_losses, val_losses, train_accuracies, val_accuracies = [], [], [], []

    for epoch in range(300):
        model.set_training(True)
        y_pred = model.forward(X_train)
        train_loss = model.compute_loss(y_train, y_pred)
        dW1, db1, dW2, db2 = model.backward(X_train, y_train, y_pred)
        optimizer.update(model, dW1, db1, dW2, db2)

        model.set_training(False)
        y_pred_val = model.forward(X_val)
        val_loss = model.compute_loss(y_val, y_pred_val)

        train_acc = np.mean((y_pred > 0.5).astype(int) == y_train)
        val_acc = np.mean((y_pred_val > 0.5).astype(int) == y_val)

        train_losses.append(train_loss)
        val_losses.append(val_loss)
        train_accuracies.append(train_acc)
        val_accuracies.append(val_acc)

        if early_stopping and early_stopping(val_loss):
            break

    return model, train_losses, val_losses, train_accuracies, val_accuracies


# =====================================================
# Comparison
# =====================================================
def compare_models(X, y):
    X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)

    configs = [
        {'name': 'Basic', 'dropout': 0.0, 'early_stop': False},
        {'name': 'With Dropout', 'dropout': 0.3, 'early_stop': False},
        {'name': 'With Early Stopping', 'dropout': 0.0, 'early_stop': True},
        {'name': 'Both Regularizations', 'dropout': 0.3, 'early_stop': True},
    ]
    optimizers = ["SGD", "Momentum", "AdaGrad", "Adam"]

    results = {}

    # Train NN with all optimizers + configs
    for config in configs:
        for opt in optimizers:
            name = f"{config['name']} + {opt}"
            print(f"\nTraining {name}...")

            model, train_losses, val_losses, train_accs, val_accs = train_model(
                X_train, y_train, X_val, y_val,
                dropout_rate=config['dropout'],
                use_early_stopping=config['early_stop'],
                optimizer_type=opt
            )

            model.set_training(False)
            y_pred_test = model.forward(X_test)
            test_accuracy = np.mean((y_pred_test > 0.5).astype(int) == y_test)

            fit_status = detect_overfitting(train_losses, val_losses)

            results[name] = {
                'train_losses': train_losses,
                'val_losses': val_losses,
                'train_accuracies': train_accs,
                'val_accuracies': val_accs,
                'test_accuracy': test_accuracy,
                'fit_status': fit_status
            }

            print(f"Test Accuracy: {test_accuracy:.4f}, Status: {fit_status}")

    # Add Gradient Boosting baseline
    print("\nTraining Gradient Boosting...")
    gb = GradientBoostingClassifier(n_estimators=200, learning_rate=0.1, max_depth=3, random_state=42)
    gb.fit(X_train, y_train.ravel())
    test_accuracy = gb.score(X_test, y_test)

    results["Gradient Boosting"] = {
        'train_losses': [],
        'val_losses': [],
        'train_accuracies': [],
        'val_accuracies': [],
        'test_accuracy': test_accuracy,
        'fit_status': "Classical ML"
    }
    print(f"Gradient Boosting Test Accuracy: {test_accuracy:.4f}")

    return results


# =====================================================
# Plotting
# =====================================================
def plot_results(results):
    # Group results by optimizer
    optimizers = ["SGD", "Momentum", "AdaGrad", "Adam"]
    configs = ["Basic", "With Dropout", "With Early Stopping", "Both Regularizations"]

    for opt in optimizers:
        fig, axes = plt.subplots(1, 2, figsize=(16, 6))
        fig.suptitle(f"Results for Optimizer: {opt}", fontsize=14, fontweight="bold")

        # Plot train/val loss curves
        for cfg in configs:
            name = f"{cfg} + {opt}"
            if name in results and len(results[name]['train_losses']) > 0:
                epochs = range(len(results[name]['train_losses']))
                axes[0].plot(epochs, results[name]['train_losses'], '--', alpha=0.6, label=f"{cfg} (Train)")
                axes[0].plot(epochs, results[name]['val_losses'], '-', alpha=0.8, label=f"{cfg} (Val)")

        axes[0].set_title('Training vs Validation Loss')
        axes[0].set_xlabel('Epoch')
        axes[0].set_ylabel('Loss')
        axes[0].legend(fontsize=8)
        axes[0].grid(True)

        # Plot test accuracy bars
        acc_names = [f"{cfg} + {opt}" for cfg in configs if f"{cfg} + {opt}" in results]
        test_accs = [results[name]['test_accuracy'] for name in acc_names]
        colors = ['red' if results[name]['fit_status'] == 'Overfitting'
                  else 'orange' if results[name]['fit_status'] == 'Underfitting'
                  else 'green' for name in acc_names]

        bars = axes[1].bar(range(len(acc_names)), test_accs, color=colors, alpha=0.7)
        axes[1].set_title('Test Accuracy by Config')
        axes[1].set_ylabel('Test Accuracy')
        axes[1].set_xticks(range(len(acc_names)))
        axes[1].set_xticklabels(acc_names, rotation=30, ha="right", fontsize=8)
        axes[1].grid(True, axis='y')

        for bar, acc in zip(bars, test_accs):
            axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                         f'{acc:.3f}', ha='center', va='bottom', fontsize=8)

        plt.tight_layout(rect=[0, 0, 1, 0.95])
        plt.show()

    # Gradient Boosting separately
    if "Gradient Boosting" in results:
        plt.figure(figsize=(6, 4))
        acc = results["Gradient Boosting"]['test_accuracy']
        plt.bar(["Gradient Boosting"], [acc], color="blue", alpha=0.7)
        plt.ylabel("Test Accuracy")
        plt.title("Gradient Boosting Baseline")
        plt.text(0, acc + 0.01, f"{acc:.3f}", ha="center", va="bottom", fontsize=8)
        plt.ylim(0, 1)
        plt.grid(True, axis="y")
        plt.show()


# =====================================================
# Extra Visualizations
# =====================================================
def extra_visualizations(results):
    configs = ["Basic", "With Dropout", "With Early Stopping", "Both Regularizations"]
    optimizers = ["SGD", "Momentum", "AdaGrad", "Adam"]

    # 1. Accuracy curves per optimizer
    for opt in optimizers:
        plt.figure(figsize=(8,5))
        for cfg in configs:
            name = f"{cfg} + {opt}"
            if name in results and len(results[name]['val_accuracies']) > 0:
                plt.plot(results[name]['val_accuracies'], label=f"{cfg} (Val)")
                plt.plot(results[name]['train_accuracies'], '--', alpha=0.6, label=f"{cfg} (Train)")
        plt.title(f"Accuracy Curves ({opt})")
        plt.xlabel("Epoch")
        plt.ylabel("Accuracy")
        plt.legend(fontsize=8)
        plt.grid(True)
        plt.show()

    # 2. Heatmap of Test Accuracies
    heatmap_data = []
    for cfg in configs:
        row = []
        for opt in optimizers:
            name = f"{cfg} + {opt}"
            row.append(results[name]['test_accuracy'] if name in results else np.nan)
        heatmap_data.append(row)

    plt.figure(figsize=(8,5))
    sns.heatmap(heatmap_data, annot=True, cmap="YlGnBu", xticklabels=optimizers, yticklabels=configs, fmt=".3f")
    plt.title("Test Accuracy Heatmap")
    plt.show()

    # 3. Overfitting Gap plot
    plt.figure(figsize=(8,5))
    gaps = []
    labels = []
    for cfg in configs:
        for opt in optimizers:
            name = f"{cfg} + {opt}"
            if name in results and results[name]['train_losses']:
                gap = results[name]['val_losses'][-1] - results[name]['train_losses'][-1]
                gaps.append(gap)
                labels.append(name)
    sns.barplot(x=gaps, y=labels, orient="h", palette="coolwarm")
    plt.title("Overfitting Gap (Val Loss - Train Loss)")
    plt.xlabel("Gap")
    plt.ylabel("Model")
    plt.show()


# =====================================================
# Run Experiment
# =====================================================
if __name__ == "__main__":
    # Load dataset
    data = load_breast_cancer()
    X = data.data
    y = data.target.reshape(-1, 1)

    # Standardize
    scaler = StandardScaler()
    X = scaler.fit_transform(X)

    # Run comparison
    results = compare_models(X, y)

    # Plot results
    plot_results(results)
    extra_visualizations(results)
